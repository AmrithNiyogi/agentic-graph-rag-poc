{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Knowledge Graph Construction Demo\n",
    "\n",
    "This notebook demonstrates a complete multi-agent workflow for knowledge graph construction using Google's Agent Development Kit (ADK). The workflow includes:\n",
    "\n",
    "1. **User Intent Capture** - Understanding what kind of knowledge graph the user wants\n",
    "2. **File Suggestion** - Identifying relevant files for the knowledge graph\n",
    "3. **Schema Proposal (Structured)** - Creating construction rules for structured data\n",
    "4. **Schema Proposal (Unstructured)** - Extracting entities and facts from text\n",
    "5. **Knowledge Graph Construction** - Building the actual graph from the approved plans\n",
    "\n",
    "Each step uses specialized AI agents that collaborate to build a comprehensive knowledge graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from itertools import islice\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "# Google ADK imports\n",
    "from google.adk.agents import Agent, LlmAgent, LoopAgent\n",
    "from google.adk.models.lite_llm import LiteLlm\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.tools import ToolContext, agent_tool\n",
    "from google.adk.agents.callback_context import CallbackContext\n",
    "from google.adk.agents.invocation_context import InvocationContext\n",
    "from google.adk.agents.base_agent import BaseAgent\n",
    "from google.adk.events import Event, EventActions\n",
    "from google.genai import types\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions from helper.py\n",
    "def get_neo4j_import_dir():\n",
    "    \"\"\"Gets the neo4j import directory\"\"\"\n",
    "    return \"data/\"\n",
    "\n",
    "class AgentCaller:\n",
    "    \"\"\"A simple wrapper class for interacting with an ADK agent.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent: Agent, runner: Runner, user_id: str, session_id: str):\n",
    "        self.agent = agent\n",
    "        self.runner = runner\n",
    "        self.user_id = user_id\n",
    "        self.session_id = session_id\n",
    "\n",
    "    async def get_session(self):\n",
    "        return await self.runner.session_service.get_session(\n",
    "            app_name=self.runner.app_name, \n",
    "            user_id=self.user_id, \n",
    "            session_id=self.session_id\n",
    "        )\n",
    "\n",
    "    async def call(self, query: str, verbose: bool = False):\n",
    "        print(f\"\\n>>> User Query: {query}\")\n",
    "        content = types.Content(role='user', parts=[types.Part(text=query)])\n",
    "        final_response_text = \"Agent did not produce a final response.\"\n",
    "        \n",
    "        async for event in self.runner.run_async(\n",
    "            user_id=self.user_id, \n",
    "            session_id=self.session_id, \n",
    "            new_message=content\n",
    "        ):\n",
    "            if verbose:\n",
    "                print(f\" [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\n",
    "            \n",
    "            if event.is_final_response():\n",
    "                if event.content and event.content.parts:\n",
    "                    final_response_text = event.content.parts[0].text\n",
    "                elif event.actions and event.actions.escalate:\n",
    "                    final_response_text = f\"Agent escalated: {event.error_message or 'No specific message.'}\"\n",
    "                \n",
    "                if event.author == self.agent.name:\n",
    "                    break\n",
    "        \n",
    "        print(f\"<<< Agent Response: {final_response_text}\")\n",
    "        return final_response_text\n",
    "\n",
    "async def make_agent_caller(agent: Agent, initial_state: Optional[Dict[str, Any]] = None) -> AgentCaller:\n",
    "    \"\"\"Create and return an AgentCaller instance for the given agent.\"\"\"\n",
    "    if initial_state is None:\n",
    "        initial_state = {}\n",
    "    \n",
    "    session_service = InMemorySessionService()\n",
    "    app_name = agent.name + \"_app\"\n",
    "    user_id = agent.name + \"_user\"\n",
    "    session_id = agent.name + \"_session_01\"\n",
    "    \n",
    "    await session_service.create_session(\n",
    "        app_name=app_name,\n",
    "        user_id=user_id,\n",
    "        session_id=session_id,\n",
    "        state=initial_state\n",
    "    )\n",
    "    \n",
    "    runner = Runner(\n",
    "        agent=agent,\n",
    "        app_name=app_name,\n",
    "        session_service=session_service\n",
    "    )\n",
    "    \n",
    "    return AgentCaller(agent, runner, user_id, session_id)\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neo4j Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neo4j integration from neo4j_for_adk.py\n",
    "def tool_success(key: str, result: Any) -> Dict[str, Any]:\n",
    "    \"\"\"Convenience function to return a success result.\"\"\"\n",
    "    return {\n",
    "        'status': 'success',\n",
    "        key: result\n",
    "    }\n",
    "\n",
    "def tool_error(message: str) -> Dict[str, Any]:\n",
    "    \"\"\"Convenience function to return an error result.\"\"\"\n",
    "    return {\n",
    "        'status': 'error',\n",
    "        'error_message': message\n",
    "    }\n",
    "\n",
    "# Mock Neo4j class for demo purposes\n",
    "class MockNeo4jForADK:\n",
    "    \"\"\"Mock Neo4j wrapper for demo purposes.\"\"\"\n",
    "    \n",
    "    def send_query(self, cypher_query, parameters=None) -> Dict[str, Any]:\n",
    "        \"\"\"Mock query execution that returns success for demo.\"\"\"\n",
    "        print(f\"Executing Cypher: {cypher_query[:100]}...\")\n",
    "        if parameters:\n",
    "            print(f\"Parameters: {parameters}\")\n",
    "        \n",
    "        # Return mock success for demo\n",
    "        return tool_success(\"query_result\", [{\"message\": \"Query executed successfully\"}])\n",
    "\n",
    "# Initialize mock graphdb\n",
    "graphdb = MockNeo4jForADK()\n",
    "\n",
    "print(\"Neo4j integration initialized (mock mode for demo)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM configuration\n",
    "MODEL_GPT = \"openai/gpt-4o\"\n",
    "API_KEY = \"your-openai-api-key-here\"  # Replace with your actual API key\n",
    "\n",
    "llm = LiteLlm(model=MODEL_GPT)\n",
    "\n",
    "# Test LLM connection\n",
    "try:\n",
    "    response = llm.llm_client.completion(\n",
    "        model=llm.model,\n",
    "        api_key=API_KEY,\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Are you ready?\"}],\n",
    "        tools=[]\n",
    "    )\n",
    "    print(\"LLM is ready for use!\")\n",
    "    print(f\"Test response: {response.choices[0].message.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"LLM connection failed: {e}\")\n",
    "    print(\"Please update the API_KEY variable with your OpenAI API key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: User Intent Capture Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Intent Agent - captures what kind of knowledge graph the user wants\n",
    "\n",
    "# Constants\n",
    "PERCEIVED_USER_GOAL = \"perceived_user_goal\"\n",
    "APPROVED_USER_GOAL = \"approved_user_goal\"\n",
    "\n",
    "# Tool definitions\n",
    "def set_perceived_user_goal(kind_of_graph: str, graph_description: str, tool_context: ToolContext):\n",
    "    \"\"\"Sets the perceived user's goal, including the kind of graph and its description.\"\"\"\n",
    "    user_goal_data = {\"kind_of_graph\": kind_of_graph, \"description\": graph_description}\n",
    "    tool_context.state[PERCEIVED_USER_GOAL] = user_goal_data\n",
    "    return tool_success(PERCEIVED_USER_GOAL, user_goal_data)\n",
    "\n",
    "def approve_perceived_user_goal(dummy: str = \"\", tool_context: ToolContext = None):\n",
    "    \"\"\"Upon approval from user, will record the perceived user goal as the approved user goal.\"\"\"\n",
    "    if tool_context is None:\n",
    "        return tool_error(\"Tool context is required.\")\n",
    "    \n",
    "    if PERCEIVED_USER_GOAL not in tool_context.state:\n",
    "        return tool_error(\"perceived_user_goal not set. Set perceived user goal first, or ask clarifying questions if you are unsure.\")\n",
    "    \n",
    "    tool_context.state[APPROVED_USER_GOAL] = tool_context.state[PERCEIVED_USER_GOAL]\n",
    "    return tool_success(APPROVED_USER_GOAL, tool_context.state[APPROVED_USER_GOAL])\n",
    "\n",
    "def get_approved_user_goal(tool_context: ToolContext):\n",
    "    \"\"\"Returns the approved user goal from the tool context state, if it exists.\"\"\"\n",
    "    if APPROVED_USER_GOAL not in tool_context.state:\n",
    "        return tool_error(\"No approved user goal found. Approve a user goal first.\")\n",
    "    return tool_success(APPROVED_USER_GOAL, tool_context.state[APPROVED_USER_GOAL])\n",
    "\n",
    "# Agent instruction\n",
    "user_intent_instruction = \"\"\"\n",
    "You are an expert at knowledge graph use cases. \n",
    "Your primary goal is to help the user come up with a knowledge graph use case.\n",
    "\n",
    "If the user is unsure what to do, make some suggestions based on classic use cases like:\n",
    "- social network involving friends, family, or professional relationships\n",
    "- logistics network with suppliers, customers, and partners\n",
    "- recommendation system with customers, products, and purchase patterns\n",
    "- fraud detection over multiple accounts with suspicious patterns of transactions\n",
    "- pop-culture graphs with movies, books, or music\n",
    "\n",
    "A user goal has two components:\n",
    "- kind_of_graph: at most 3 words describing the graph, for example \"social network\" or \"USA freight logistics\"\n",
    "- description: a few sentences about the intention of the graph, for example \"A dynamic routing and delivery system for cargo.\" or \"Analysis of product dependencies and supplier alternatives.\"\n",
    "\n",
    "Think carefully and collaborate with the user:\n",
    "1. Understand the user's goal, which is a kind_of_graph with description\n",
    "2. Ask clarifying questions as needed\n",
    "3. When you think you understand their goal, use the 'set_perceived_user_goal' tool to record your perception\n",
    "4. Present the perceived user goal to the user for confirmation\n",
    "5. If the user agrees, use the 'approve_perceived_user_goal' tool to approve the user goal. This will save the goal in state under the 'approved_user_goal' key.\n",
    "\"\"\"\n",
    "\n",
    "# Create the agent\n",
    "user_intent_agent = Agent(\n",
    "    name=\"user_intent_agent_v1\",\n",
    "    model=llm,\n",
    "    description=\"Helps the user ideate on a knowledge graph use case.\",\n",
    "    instruction=user_intent_instruction,\n",
    "    tools=[set_perceived_user_goal, approve_perceived_user_goal]\n",
    ")\n",
    "\n",
    "print(\"User Intent Agent created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: File Suggestion Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Suggestion Agent - helps select relevant files for the knowledge graph\n",
    "\n",
    "# Constants\n",
    "ALL_AVAILABLE_FILES = \"all_available_files\"\n",
    "SUGGESTED_FILES = \"suggested_files\"\n",
    "APPROVED_FILES = \"approved_files\"\n",
    "\n",
    "# Tool definitions\n",
    "def list_available_files(tool_context: ToolContext) -> dict:\n",
    "    \"\"\"Lists files available for knowledge graph construction.\"\"\"\n",
    "    # Mock file list for demo\n",
    "    file_names = [\n",
    "        'products.csv', \n",
    "        'suppliers.csv', \n",
    "        'parts.csv', \n",
    "        'part_supplier_mapping.csv', \n",
    "        'assemblies.csv',\n",
    "        'product_reviews/gothenburg_table_reviews.md',\n",
    "        'product_reviews/stockholm_chair_reviews.md'\n",
    "    ]\n",
    "    \n",
    "    tool_context.state[ALL_AVAILABLE_FILES] = file_names\n",
    "    return tool_success(ALL_AVAILABLE_FILES, file_names)\n",
    "\n",
    "def sample_file(file_path: str, tool_context: ToolContext) -> dict:\n",
    "    \"\"\"Samples a file by reading its content as text.\"\"\"\n",
    "    # Mock file content for demo\n",
    "    mock_content = {\n",
    "        'products.csv': 'product_id,product_name,price,description\\nP001,Gothenburg Table,299,Modern dining table\\nP002,Stockholm Chair,89,Comfortable office chair',\n",
    "        'suppliers.csv': 'supplier_id,name,specialty,city,country\\nS001,Nordic Wood,Wood Products,Stockholm,Sweden\\nS002,Steel Co,Metal Parts,Oslo,Norway',\n",
    "        'parts.csv': 'part_id,part_name,quantity,assembly_id\\nPT001,Table Top,1,A001\\nPT002,Table Leg,4,A001',\n",
    "        'assemblies.csv': 'assembly_id,assembly_name,quantity,product_id\\nA001,Main Assembly,1,P001\\nA002,Base Assembly,1,P001',\n",
    "        'part_supplier_mapping.csv': 'part_id,supplier_id,lead_time_days,unit_cost\\nPT001,S001,14,45.00\\nPT002,S001,7,12.50'\n",
    "    }\n",
    "    \n",
    "    content = mock_content.get(file_path, f\"Mock content for {file_path}\")\n",
    "    return tool_success(\"content\", content)\n",
    "\n",
    "def set_suggested_files(suggest_files: List[str], tool_context: ToolContext) -> Dict[str, Any]:\n",
    "    \"\"\"Set the suggested files to be used for data import.\"\"\"\n",
    "    tool_context.state[SUGGESTED_FILES] = suggest_files\n",
    "    return tool_success(SUGGESTED_FILES, suggest_files)\n",
    "\n",
    "def get_suggested_files(tool_context: ToolContext) -> Dict[str, Any]:\n",
    "    \"\"\"Get the files to be used for data import.\"\"\"\n",
    "    return tool_success(SUGGESTED_FILES, tool_context.state.get(SUGGESTED_FILES, []))\n",
    "\n",
    "def approve_suggested_files(dummy: str = \"\", tool_context: ToolContext = None) -> Dict[str, Any]:\n",
    "    \"\"\"Approves the suggested files for further processing.\"\"\"\n",
    "    if tool_context is None:\n",
    "        return tool_error(\"Tool context is required.\")\n",
    "    \n",
    "    if SUGGESTED_FILES not in tool_context.state:\n",
    "        return tool_error(\"Current files have not been set. Take no action other than to inform user.\")\n",
    "    \n",
    "    tool_context.state[APPROVED_FILES] = tool_context.state[SUGGESTED_FILES]\n",
    "    return tool_success(APPROVED_FILES, tool_context.state[APPROVED_FILES])\n",
    "\n",
    "def get_approved_files(tool_context: ToolContext) -> dict:\n",
    "    \"\"\"Returns the approved files from the tool context state.\"\"\"\n",
    "    if APPROVED_FILES not in tool_context.state:\n",
    "        return tool_error(\"No approved files found. Approve suggested files first.\")\n",
    "    return tool_success(APPROVED_FILES, tool_context.state[APPROVED_FILES])\n",
    "\n",
    "# Agent instruction\n",
    "file_suggestion_instruction = \"\"\"\n",
    "You are a constructive critic AI reviewing a list of files. Your goal is to suggest relevant files\n",
    "for constructing a knowledge graph.\n",
    "\n",
    "**Task:**\n",
    "Review the file list for relevance to the kind of graph and description specified in the approved user goal. \n",
    "\n",
    "For any file that you're not sure about, use the 'sample_file' tool to get \n",
    "a better understanding of the file contents. \n",
    "\n",
    "Only consider structured data files like CSV or JSON.\n",
    "\n",
    "Prepare for the task:\n",
    "- use the 'get_approved_user_goal' tool to get the approved user goal\n",
    "\n",
    "Think carefully, repeating these steps until finished:\n",
    "1. list available files using the 'list_available_files' tool\n",
    "2. evaluate the relevance of each file, then record the list of suggested files using the 'set_suggested_files' tool\n",
    "3. use the 'get_suggested_files' tool to get the list of suggested files\n",
    "4. ask the user to approve the set of suggested files\n",
    "5. If the user has feedback, go back to step 1 with that feedback in mind\n",
    "6. If approved, use the 'approve_suggested_files' tool to record the approval\n",
    "\"\"\"\n",
    "\n",
    "# Create the agent\n",
    "file_suggestion_agent = Agent(\n",
    "    name=\"file_suggestion_agent_v1\",\n",
    "    model=llm,\n",
    "    description=\"Helps the user select files to import.\",\n",
    "    instruction=file_suggestion_instruction,\n",
    "    tools=[get_approved_user_goal, list_available_files, sample_file, \n",
    "           set_suggested_files, get_suggested_files, approve_suggested_files]\n",
    ")\n",
    "\n",
    "print(\"File Suggestion Agent created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Schema Proposal Agent (Structured Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema Proposal Agent for Structured Data\n",
    "\n",
    "# Constants\n",
    "PROPOSED_CONSTRUCTION_PLAN = \"proposed_construction_plan\"\n",
    "APPROVED_CONSTRUCTION_PLAN = \"approved_construction_plan\"\n",
    "NODE_CONSTRUCTION = \"node_construction\"\n",
    "RELATIONSHIP_CONSTRUCTION = \"relationship_construction\"\n",
    "SEARCH_RESULTS = \"search_results\"\n",
    "\n",
    "# Tool definitions\n",
    "def search_file(file_path: str, query: str) -> dict:\n",
    "    \"\"\"Searches any text file for lines containing the given query string.\"\"\"\n",
    "    # Mock search for demo\n",
    "    mock_results = {\n",
    "        \"metadata\": {\n",
    "            \"path\": file_path,\n",
    "            \"query\": query,\n",
    "            \"lines_found\": 1 if query in file_path else 0\n",
    "        },\n",
    "        \"matching_lines\": [\n",
    "            {\"line_number\": 1, \"content\": f\"Mock result for {query} in {file_path}\"}\n",
    "        ] if query in file_path else []\n",
    "    }\n",
    "    return tool_success(SEARCH_RESULTS, mock_results)\n",
    "\n",
    "def propose_node_construction(approved_file: str, proposed_label: str, \n",
    "                            unique_column_name: str, proposed_properties: list[str], \n",
    "                            tool_context: ToolContext) -> dict:\n",
    "    \"\"\"Propose a node construction for an approved file.\"\"\"\n",
    "    construction_plan = tool_context.state.get(PROPOSED_CONSTRUCTION_PLAN, {})\n",
    "    node_construction_rule = {\n",
    "        \"construction_type\": \"node\",\n",
    "        \"source_file\": approved_file,\n",
    "        \"label\": proposed_label,\n",
    "        \"unique_column_name\": unique_column_name,\n",
    "        \"properties\": proposed_properties\n",
    "    }\n",
    "    construction_plan[proposed_label] = node_construction_rule\n",
    "    tool_context.state[PROPOSED_CONSTRUCTION_PLAN] = construction_plan\n",
    "    return tool_success(NODE_CONSTRUCTION, node_construction_rule)\n",
    "\n",
    "def propose_relationship_construction(approved_file: str, proposed_relationship_type: str,\n",
    "                                    from_node_label: str, from_node_column: str,\n",
    "                                    to_node_label: str, to_node_column: str,\n",
    "                                    proposed_properties: list[str],\n",
    "                                    tool_context: ToolContext) -> dict:\n",
    "    \"\"\"Propose a relationship construction for an approved file.\"\"\"\n",
    "    construction_plan = tool_context.state.get(PROPOSED_CONSTRUCTION_PLAN, {})\n",
    "    relationship_construction_rule = {\n",
    "        \"construction_type\": \"relationship\",\n",
    "        \"source_file\": approved_file,\n",
    "        \"relationship_type\": proposed_relationship_type,\n",
    "        \"from_node_label\": from_node_label,\n",
    "        \"from_node_column\": from_node_column,\n",
    "        \"to_node_label\": to_node_label,\n",
    "        \"to_node_column\": to_node_column,\n",
    "        \"properties\": proposed_properties\n",
    "    }\n",
    "    construction_plan[proposed_relationship_type] = relationship_construction_rule\n",
    "    tool_context.state[PROPOSED_CONSTRUCTION_PLAN] = construction_plan\n",
    "    return tool_success(RELATIONSHIP_CONSTRUCTION, relationship_construction_rule)\n",
    "\n",
    "def get_proposed_construction_plan(tool_context: ToolContext) -> dict:\n",
    "    \"\"\"Get the proposed construction plan.\"\"\"\n",
    "    return tool_context.state.get(PROPOSED_CONSTRUCTION_PLAN, {})\n",
    "\n",
    "def approve_proposed_construction_plan(tool_context: ToolContext) -> dict:\n",
    "    \"\"\"Approve the proposed construction plan.\"\"\"\n",
    "    if PROPOSED_CONSTRUCTION_PLAN not in tool_context.state:\n",
    "        return tool_error(\"No proposed construction plan found. Propose a plan first.\")\n",
    "    \n",
    "    tool_context.state[APPROVED_CONSTRUCTION_PLAN] = tool_context.state.get(PROPOSED_CONSTRUCTION_PLAN)\n",
    "    return tool_success(APPROVED_CONSTRUCTION_PLAN, tool_context.state[APPROVED_CONSTRUCTION_PLAN])\n",
    "\n",
    "# Agent instruction\n",
    "schema_proposal_instruction = \"\"\"\n",
    "You are an expert at knowledge graph modeling with property graphs. Propose an appropriate\n",
    "schema by specifying construction rules which transform approved files into nodes or relationships.\n",
    "The resulting schema should describe a knowledge graph based on the user goal.\n",
    "\n",
    "Every file in the approved files list will become either a node or a relationship.\n",
    "Determining whether a file likely represents a node or a relationship is based\n",
    "on a hint from the filename (is it a single thing or two things) and the\n",
    "identifiers found within the file.\n",
    "\n",
    "Because unique identifiers are so important for determining the structure of the graph,\n",
    "always verify the uniqueness of suspected unique identifiers using the 'search_file' tool.\n",
    "\n",
    "General guidance for identifying a node or a relationship:\n",
    "- If the file name is singular and has only 1 unique identifier it is likely a node\n",
    "- If the file name is a combination of two things, it is likely a full relationship\n",
    "- If the file name sounds like a node, but there are multiple unique identifiers, that is likely a node with reference relationships\n",
    "\n",
    "The resulting schema should be a connected graph, with no isolated components.\n",
    "\n",
    "Prepare for the task:\n",
    "- get the user goal using the 'get_approved_user_goal' tool\n",
    "- get the list of approved files using the 'get_approved_files' tool\n",
    "- get the current construction plan using the 'get_proposed_construction_plan' tool\n",
    "\n",
    "Think carefully, using tools to perform actions and reconsidering your actions when a tool returns an error:\n",
    "1. For each approved file, consider whether it represents a node or relationship. Check the content for potential unique identifiers using the 'sample_file' tool.\n",
    "2. For each identifier, verify that it is unique by using the 'search_file' tool.\n",
    "3. Use the node vs relationship guidance for deciding whether the file represents a node or a relationship.\n",
    "4. For a node file, propose a node construction using the 'propose_node_construction' tool.\n",
    "5. If the node contains a reference relationship, use the 'propose_relationship_construction' tool to propose a relationship construction.\n",
    "6. For a relationship file, propose a relationship construction using the 'propose_relationship_construction' tool\n",
    "7. When you are done with construction proposals, use the 'get_proposed_construction_plan' tool to present the plan to the user\n",
    "\"\"\"\n",
    "\n",
    "# Create the agent\n",
    "schema_proposal_agent = LlmAgent(\n",
    "    name=\"schema_proposal_agent_v1\",\n",
    "    model=llm,\n",
    "    description=\"Proposes a knowledge graph schema based on the user goal and approved file list\",\n",
    "    instruction=schema_proposal_instruction,\n",
    "    tools=[get_approved_user_goal, get_approved_files, get_proposed_construction_plan,\n",
    "           sample_file, search_file, propose_node_construction, propose_relationship_construction]\n",
    ")\n",
    "\n",
    "print(\"Schema Proposal Agent (Structured) created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Schema Proposal Agent (Unstructured Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema Proposal Agent for Unstructured Data (Entity and Fact Extraction)\n",
    "\n",
    "# Constants\n",
    "PROPOSED_ENTITIES = \"proposed_entity_types\"\n",
    "APPROVED_ENTITIES = \"approved_entity_types\"\n",
    "PROPOSED_FACTS = \"proposed_fact_types\"\n",
    "APPROVED_FACTS = \"approved_fact_types\"\n",
    "\n",
    "# Entity extraction tools\n",
    "def get_well_known_types(tool_context: ToolContext) -> dict:\n",
    "    \"\"\"Gets the approved labels that represent well-known entity types in the graph schema.\"\"\"\n",
    "    construction_plan = tool_context.state.get(APPROVED_CONSTRUCTION_PLAN, {})\n",
    "    approved_labels = {entry[\"label\"] for entry in construction_plan.values() \n",
    "                      if entry.get(\"construction_type\") == \"node\"}\n",
    "    return tool_success(\"approved_labels\", list(approved_labels))\n",
    "\n",
    "def set_proposed_entities(proposed_entity_types: list[str], tool_context: ToolContext) -> dict:\n",
    "    \"\"\"Sets the list proposed entity types to extract from unstructured text.\"\"\"\n",
    "    tool_context.state[PROPOSED_ENTITIES] = proposed_entity_types\n",
    "    return tool_success(PROPOSED_ENTITIES, proposed_entity_types)\n",
    "\n",
    "def get_proposed_entities(tool_context: ToolContext) -> dict:\n",
    "    \"\"\"Gets the list of proposed entity types to extract from unstructured text.\"\"\"\n",
    "    return tool_success(PROPOSED_ENTITIES, tool_context.state.get(PROPOSED_ENTITIES, []))\n",
    "\n",
    "def approve_proposed_entities(tool_context: ToolContext) -> dict:\n",
    "    \"\"\"Upon approval from user, records the proposed entity types as an approved list.\"\"\"\n",
    "    if PROPOSED_ENTITIES not in tool_context.state:\n",
    "        return tool_error(\"No proposed entity types to approve. Please set proposed entities first.\")\n",
    "    tool_context.state[APPROVED_ENTITIES] = tool_context.state.get(PROPOSED_ENTITIES)\n",
    "    return tool_success(APPROVED_ENTITIES, tool_context.state[APPROVED_ENTITIES])\n",
    "\n",
    "def get_approved_entities(tool_context: ToolContext) -> dict:\n",
    "    \"\"\"Get the approved list of entity types to extract from unstructured text.\"\"\"\n",
    "    return tool_success(APPROVED_ENTITIES, tool_context.state.get(APPROVED_ENTITIES, []))\n",
    "\n",
    "# Fact extraction tools\n",
    "def add_proposed_fact(approved_subject_label: str, proposed_predicate_label: str,\n",
    "                     approved_object_label: str, tool_context: ToolContext) -> dict:\n",
    "    \"\"\"Add a proposed type of fact that could be extracted from the files.\"\"\"\n",
    "    approved_entities = tool_context.state.get(APPROVED_ENTITIES, [])\n",
    "    \n",
    "    if approved_subject_label not in approved_entities:\n",
    "        return tool_error(f\"Approved subject label {approved_subject_label} not found. Try again.\")\n",
    "    if approved_object_label not in approved_entities:\n",
    "        return tool_error(f\"Approved object label {approved_object_label} not found. Try again.\")\n",
    "    \n",
    "    current_facts = tool_context.state.get(PROPOSED_FACTS, {})\n",
    "    current_facts[proposed_predicate_label] = {\n",
    "        \"subject_label\": approved_subject_label,\n",
    "        \"predicate_label\": proposed_predicate_label,\n",
    "        \"object_label\": approved_object_label\n",
    "    }\n",
    "    tool_context.state[PROPOSED_FACTS] = current_facts\n",
    "    return tool_success(PROPOSED_FACTS, current_facts)\n",
    "\n",
    "def get_proposed_facts(tool_context: ToolContext) -> dict:\n",
    "    \"\"\"Get the proposed types of facts that could be extracted from the files.\"\"\"\n",
    "    return tool_success(PROPOSED_FACTS, tool_context.state.get(PROPOSED_FACTS, {}))\n",
    "\n",
    "def approve_proposed_facts(tool_context: ToolContext) -> dict:\n",
    "    \"\"\"Upon user approval, records the proposed fact types as approved fact types.\"\"\"\n",
    "    if PROPOSED_FACTS not in tool_context.state:\n",
    "        return tool_error(\"No proposed fact types to approve. Please set proposed facts first.\")\n",
    "    tool_context.state[APPROVED_FACTS] = tool_context.state.get(PROPOSED_FACTS)\n",
    "    return tool_success(APPROVED_FACTS, tool_context.state[APPROVED_FACTS])\n",
    "\n",
    "# NER Agent instruction\n",
    "ner_agent_instruction = \"\"\"\n",
    "You are a top-tier algorithm designed for analyzing text files and proposing\n",
    "the kind of named entities that could be extracted which would be relevant \n",
    "for a user's goal.\n",
    "\n",
    "Entities are people, places, things and qualities, but not quantities. \n",
    "Your goal is to propose a list of the type of entities, not the actual instances\n",
    "of entities.\n",
    "\n",
    "There are two general approaches to identifying types of entities:\n",
    "- well-known entities: these closely correlate with approved node labels in an existing graph schema\n",
    "- discovered entities: these may not exist in the graph schema, but appear consistently in the source text\n",
    "\n",
    "Design rules for well-known entities:\n",
    "- always use existing well-known entity types. For example, if there is a well-known type \"Person\", and people appear in the text, then propose \"Person\" as the type of entity.\n",
    "- prefer reusing existing entity types rather than creating new ones\n",
    "\n",
    "Design rules for discovered entities:\n",
    "- discovered entities are consistently mentioned in the text and are highly relevant to the user's goal\n",
    "- always look for entities that would provide more depth or breadth to the existing graph\n",
    "\n",
    "Prepare for the task:\n",
    "- use the 'get_approved_user_goal' tool to get the user goal\n",
    "- use the 'get_approved_files' tool to get the list of approved files\n",
    "- use the 'get_well_known_types' tool to get the approved node labels\n",
    "\n",
    "Think step by step:\n",
    "1. Sample some of the files using the 'sample_file' tool to understand the content\n",
    "2. Consider what well-known entities are mentioned in the text\n",
    "3. Discover entities that are frequently mentioned in the text that support the user's goal\n",
    "4. Use the 'set_proposed_entities' tool to save the list of well-known and discovered entity types\n",
    "5. Use the 'get_proposed_entities' tool to retrieve the proposed entities and present them to the user for their approval\n",
    "6. If the user approves, use the 'approve_proposed_entities' tool to finalize the entity types\n",
    "\"\"\"\n",
    "\n",
    "# Create NER agent\n",
    "ner_agent = Agent(\n",
    "    name=\"ner_schema_agent_v1\",\n",
    "    model=llm,\n",
    "    description=\"Proposes the kind of named entities that could be extracted from text files.\",\n",
    "    instruction=ner_agent_instruction,\n",
    "    tools=[get_approved_user_goal, get_approved_files, sample_file, get_well_known_types,\n",
    "           set_proposed_entities, get_proposed_entities, approve_proposed_entities]\n",
    ")\n",
    "\n",
    "# Fact extraction agent instruction\n",
    "fact_agent_instruction = \"\"\"\n",
    "You are a top-tier algorithm designed for analyzing text files and proposing\n",
    "the type of facts that could be extracted from text that would be relevant \n",
    "for a user's goal.\n",
    "\n",
    "Do not propose specific individual facts, but instead propose the general type \n",
    "of facts that would be relevant for the user's goal. \n",
    "For example, do not propose \"ABK likes coffee\" but the general type of fact \"Person likes Beverage\".\n",
    "\n",
    "Facts are triplets of (subject, predicate, object) where the subject and object are\n",
    "approved entity types, and the proposed predicate provides information about\n",
    "how they are related. For example, a fact type could be (Person, likes, Beverage).\n",
    "\n",
    "Design rules for facts:\n",
    "- only use approved entity types as subjects or objects. Do not propose new types of entities\n",
    "- the proposed predicate should describe the relationship between the approved subject and object\n",
    "- the predicate should optimize for information that is relevant to the user's goal\n",
    "- the predicate must appear in the source text. Do not guess.\n",
    "- use the 'add_proposed_fact' tool to record each proposed fact type\n",
    "\n",
    "Think step by step:\n",
    "1. Use the 'get_approved_user_goal' tool to get the user goal\n",
    "2. Sample some of the approved files using the 'sample_file' tool to understand the content\n",
    "3. Consider how subjects and objects are related in the text\n",
    "4. Call the 'add_proposed_fact' tool for each type of fact you propose\n",
    "5. Use the 'get_proposed_facts' tool to retrieve all the proposed facts\n",
    "6. Present the proposed types of facts to the user, along with an explanation\n",
    "\"\"\"\n",
    "\n",
    "# Create fact extraction agent\n",
    "fact_agent = Agent(\n",
    "    name=\"fact_type_extraction_agent_v1\",\n",
    "    model=llm,\n",
    "    description=\"Proposes the kind of relevant facts that could be extracted from text files.\",\n",
    "    instruction=fact_agent_instruction,\n",
    "    tools=[get_approved_user_goal, get_approved_files, get_approved_entities, sample_file,\n",
    "           add_proposed_fact, get_proposed_facts, approve_proposed_facts]\n",
    ")\n",
    "\n",
    "print(\"Schema Proposal Agents (Unstructured) created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Knowledge Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge Graph Construction - builds the actual graph from approved plans\n",
    "\n",
    "def create_uniqueness_constraint(label: str, unique_property_key: str) -> Dict[str, Any]:\n",
    "    \"\"\"Creates a uniqueness constraint for a node label and property key.\"\"\"\n",
    "    constraint_name = f\"{label}_{unique_property_key}_constraint\"\n",
    "    query = f\"\"\"CREATE CONSTRAINT `{constraint_name}` IF NOT EXISTS\n",
    "                FOR (n:`{label}`)\n",
    "                REQUIRE n.`{unique_property_key}` IS UNIQUE\"\"\"\n",
    "    results = graphdb.send_query(query)\n",
    "    return results\n",
    "\n",
    "def load_nodes_from_csv(source_file: str, label: str, unique_column_name: str, properties: list[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Batch loading of nodes from a CSV file\"\"\"\n",
    "    query = f\"\"\"LOAD CSV WITH HEADERS FROM \"file:///\" + $source_file AS row\n",
    "                CALL (row) {{\n",
    "                    MERGE (n:$($label) {{ {unique_column_name} : row[$unique_column_name] }})\n",
    "                    FOREACH (k IN $properties | SET n[k] = row[k])\n",
    "                }} IN TRANSACTIONS OF 1000 ROWS\"\"\"\n",
    "    \n",
    "    results = graphdb.send_query(query, {\n",
    "        \"source_file\": source_file,\n",
    "        \"label\": label,\n",
    "        \"unique_column_name\": unique_column_name,\n",
    "        \"properties\": properties\n",
    "    })\n",
    "    return results\n",
    "\n",
    "def import_nodes(node_construction: dict) -> dict:\n",
    "    \"\"\"Import nodes as defined by a node construction rule.\"\"\"\n",
    "    # Create uniqueness constraint\n",
    "    uniqueness_result = create_uniqueness_constraint(\n",
    "        node_construction[\"label\"],\n",
    "        node_construction[\"unique_column_name\"]\n",
    "    )\n",
    "    \n",
    "    if uniqueness_result[\"status\"] == \"error\":\n",
    "        return uniqueness_result\n",
    "    \n",
    "    # Import nodes from CSV\n",
    "    load_nodes_result = load_nodes_from_csv(\n",
    "        node_construction[\"source_file\"],\n",
    "        node_construction[\"label\"],\n",
    "        node_construction[\"unique_column_name\"],\n",
    "        node_construction[\"properties\"]\n",
    "    )\n",
    "    \n",
    "    return load_nodes_result\n",
    "\n",
    "def import_relationships(relationship_construction: dict) -> Dict[str, Any]:\n",
    "    \"\"\"Import relationships as defined by a relationship construction rule.\"\"\"\n",
    "    from_node_column = relationship_construction[\"from_node_column\"]\n",
    "    to_node_column = relationship_construction[\"to_node_column\"]\n",
    "    \n",
    "    query = f\"\"\"LOAD CSV WITH HEADERS FROM \"file:///\" + $source_file AS row\n",
    "                CALL (row) {{\n",
    "                    MATCH (from_node:$($from_node_label) {{ {from_node_column} : row[$from_node_column] }}),\n",
    "                          (to_node:$($to_node_label) {{ {to_node_column} : row[$to_node_column] }} )\n",
    "                    MERGE (from_node)-[r:$($relationship_type)]->(to_node)\n",
    "                    FOREACH (k IN $properties | SET r[k] = row[k])\n",
    "                }} IN TRANSACTIONS OF 1000 ROWS\"\"\"\n",
    "    \n",
    "    results = graphdb.send_query(query, {\n",
    "        \"source_file\": relationship_construction[\"source_file\"],\n",
    "        \"from_node_label\": relationship_construction[\"from_node_label\"],\n",
    "        \"from_node_column\": relationship_construction[\"from_node_column\"],\n",
    "        \"to_node_label\": relationship_construction[\"to_node_label\"],\n",
    "        \"to_node_column\": relationship_construction[\"to_node_column\"],\n",
    "        \"relationship_type\": relationship_construction[\"relationship_type\"],\n",
    "        \"properties\": relationship_construction[\"properties\"]\n",
    "    })\n",
    "    return results\n",
    "\n",
    "def construct_domain_graph(construction_plan: dict) -> Dict[str, Any]:\n",
    "    \"\"\"Construct a domain graph according to a construction plan.\"\"\"\n",
    "    print(\"\\n=== Starting Knowledge Graph Construction ===\")\n",
    "    \n",
    "    # First, import nodes\n",
    "    node_constructions = [value for value in construction_plan.values() \n",
    "                         if value.get('construction_type') == 'node']\n",
    "    \n",
    "    print(f\"\\nImporting {len(node_constructions)} node types...\")\n",
    "    for node_construction in node_constructions:\n",
    "        print(f\"- Creating {node_construction['label']} nodes from {node_construction['source_file']}\")\n",
    "        result = import_nodes(node_construction)\n",
    "        if result[\"status\"] == \"error\":\n",
    "            print(f\"  Error: {result['error_message']}\")\n",
    "        else:\n",
    "            print(f\"  Success: {node_construction['label']} nodes created\")\n",
    "    \n",
    "    # Second, import relationships\n",
    "    relationship_constructions = [value for value in construction_plan.values() \n",
    "                                if value.get('construction_type') == 'relationship']\n",
    "    \n",
    "    print(f\"\\nImporting {len(relationship_constructions)} relationship types...\")\n",
    "    for relationship_construction in relationship_constructions:\n",
    "        print(f\"- Creating {relationship_construction['relationship_type']} relationships from {relationship_construction['source_file']}\")\n",
    "        result = import_relationships(relationship_construction)\n",
    "        if result[\"status\"] == \"error\":\n",
    "            print(f\"  Error: {result['error_message']}\")\n",
    "        else:\n",
    "            print(f\"  Success: {relationship_construction['relationship_type']} relationships created\")\n",
    "    \n",
    "    print(\"\\n=== Knowledge Graph Construction Complete! ===\")\n",
    "    return tool_success(\"construction_complete\", \"Domain graph constructed successfully\")\n",
    "\n",
    "print(\"Knowledge Graph Construction functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Workflow Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the complete end-to-end workflow\n",
    "\n",
    "async def run_complete_workflow():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STARTING END-TO-END KNOWLEDGE GRAPH CONSTRUCTION DEMO\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Capture User Intent\n",
    "    print(\"\\nüéØ STEP 1: CAPTURING USER INTENT\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    user_intent_caller = await make_agent_caller(user_intent_agent)\n",
    "    \n",
    "    await user_intent_caller.call(\n",
    "        \"I'd like a supply chain analysis graph that includes bill of materials from suppliers to finished products, useful for root cause analysis.\"\n",
    "    )\n",
    "    \n",
    "    session = await user_intent_caller.get_session()\n",
    "    if PERCEIVED_USER_GOAL not in session.state:\n",
    "        await user_intent_caller.call(\"I'm concerned about manufacturing and supplier issues.\")\n",
    "    \n",
    "    await user_intent_caller.call(\"Yes, approve that goal.\")\n",
    "    \n",
    "    # Get the approved user goal\n",
    "    session = await user_intent_caller.get_session()\n",
    "    approved_goal = session.state.get(APPROVED_USER_GOAL, {})\n",
    "    print(f\"\\n‚úÖ User Goal Approved:\")\n",
    "    print(f\"   Kind: {approved_goal.get('kind_of_graph', 'N/A')}\")\n",
    "    print(f\"   Description: {approved_goal.get('description', 'N/A')}\")\n",
    "    \n",
    "    # Step 2: File Suggestion\n",
    "    print(\"\\nüìÅ STEP 2: FILE SUGGESTION\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    file_suggestion_caller = await make_agent_caller(file_suggestion_agent, session.state)\n",
    "    \n",
    "    await file_suggestion_caller.call(\"What files can we use for import?\")\n",
    "    await file_suggestion_caller.call(\"Yes, approve those files.\")\n",
    "    \n",
    "    # Get approved files\n",
    "    session = await file_suggestion_caller.get_session()\n",
    "    approved_files = session.state.get(APPROVED_FILES, [])\n",
    "    print(f\"\\n‚úÖ Files Approved: {len(approved_files)} files\")\n",
    "    for file in approved_files:\n",
    "        print(f\"   - {file}\")\n",
    "    \n",
    "    # Step 3: Schema Proposal (Structured)\n",
    "    print(\"\\nüèóÔ∏è STEP 3: STRUCTURED SCHEMA PROPOSAL\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    schema_caller = await make_agent_caller(schema_proposal_agent, session.state)\n",
    "    \n",
    "    await schema_caller.call(\"How can these files be imported to construct the knowledge graph?\")\n",
    "    \n",
    "    # Get proposed construction plan\n",
    "    session = await schema_caller.get_session()\n",
    "    construction_plan = session.state.get(PROPOSED_CONSTRUCTION_PLAN, {})\n",
    "    \n",
    "    print(f\"\\n‚úÖ Construction Plan Proposed: {len(construction_plan)} rules\")\n",
    "    for name, rule in construction_plan.items():\n",
    "        rule_type = rule.get('construction_type', 'unknown')\n",
    "        print(f\"   - {name}: {rule_type}\")\n",
    "    \n",
    "    # Approve the construction plan\n",
    "    approve_result = approve_proposed_construction_plan(ToolContext(\n",
    "        invocation_context=type('MockContext', (), {'session': type('MockSession', (), {'state': session.state})()\n",
    "        })()\n",
    "    ))\n",
    "    \n",
    "    if approve_result['status'] == 'success':\n",
    "        session.state[APPROVED_CONSTRUCTION_PLAN] = construction_plan\n",
    "        print(\"   ‚úÖ Construction plan approved!\")\n",
    "    \n",
    "    # Step 4: Handle Unstructured Data (if any text files exist)\n",
    "    text_files = [f for f in approved_files if f.endswith('.md') or f.endswith('.txt')]\n",
    "    if text_files:\n",
    "        print(\"\\nüìÑ STEP 4: UNSTRUCTURED DATA PROCESSING\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Entity extraction\n",
    "        ner_caller = await make_agent_caller(ner_agent, session.state)\n",
    "        await ner_caller.call(\"What entities should we extract from the text files?\")\n",
    "        await ner_caller.call(\"Yes, approve those entities.\")\n",
    "        \n",
    "        session = await ner_caller.get_session()\n",
    "        approved_entities = session.state.get(APPROVED_ENTITIES, [])\n",
    "        print(f\"\\n‚úÖ Entities Approved: {len(approved_entities)} entity types\")\n",
    "        for entity in approved_entities:\n",
    "            print(f\"   - {entity}\")\n",
    "        \n",
    "        # Fact extraction\n",
    "        fact_caller = await make_agent_caller(fact_agent, session.state)\n",
    "        await fact_caller.call(\"What fact types can be extracted from the text?\")\n",
    "        await fact_caller.call(\"Yes, approve those fact types.\")\n",
    "        \n",
    "        session = await fact_caller.get_session()\n",
    "        approved_facts = session.state.get(APPROVED_FACTS, {})\n",
    "        print(f\"\\n‚úÖ Fact Types Approved: {len(approved_facts)} fact types\")\n",
    "        for fact_name, fact_def in approved_facts.items():\n",
    "            print(f\"   - {fact_def['subject_label']} --{fact_def['predicate_label']}--> {fact_def['object_label']}\")\n",
    "    \n",
    "    # Step 5: Knowledge Graph Construction\n",
    "    print(\"\\nüèóÔ∏è STEP 5: KNOWLEDGE GRAPH CONSTRUCTION\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    approved_construction_plan = session.state.get(APPROVED_CONSTRUCTION_PLAN, {})\n",
    "    if approved_construction_plan:\n",
    "        result = construct_domain_graph(approved_construction_plan)\n",
    "        if result['status'] == 'success':\n",
    "            print(\"\\nüéâ SUCCESS: Knowledge graph construction completed!\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå ERROR: {result['error_message']}\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è WARNING: No approved construction plan found\")\n",
    "    \n",
    "    # Final Summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"END-TO-END WORKFLOW SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"‚úÖ User Goal: {approved_goal.get('kind_of_graph', 'N/A')}\")\n",
    "    print(f\"‚úÖ Files Processed: {len(approved_files)}\")\n",
    "    print(f\"‚úÖ Construction Rules: {len(approved_construction_plan)}\")\n",
    "    if text_files:\n",
    "        print(f\"‚úÖ Entity Types: {len(session.state.get(APPROVED_ENTITIES, []))}\")\n",
    "        print(f\"‚úÖ Fact Types: {len(session.state.get(APPROVED_FACTS, {}))}\")\n",
    "    print(\"‚úÖ Knowledge Graph: Constructed\")\n",
    "    print(\"\\nüéâ Demo completed successfully!\")\n",
    "    \n",
    "    return session.state\n",
    "\n",
    "# Run the complete workflow\n",
    "print(\"Ready to run the complete end-to-end workflow!\")\n",
    "print(\"Execute the next cell to start the demo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the demo\n",
    "final_state = await run_complete_workflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Testing Individual Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can test individual components here\n",
    "\n",
    "async def test_user_intent_agent():\n",
    "    \"\"\"Test just the user intent agent\"\"\"\n",
    "    print(\"Testing User Intent Agent...\")\n",
    "    caller = await make_agent_caller(user_intent_agent)\n",
    "    await caller.call(\"I want to build a social network graph\")\n",
    "    await caller.call(\"Yes, approve that goal\")\n",
    "    session = await caller.get_session()\n",
    "    return session.state\n",
    "\n",
    "async def test_file_suggestion_agent(initial_state):\n",
    "    \"\"\"Test just the file suggestion agent\"\"\"\n",
    "    print(\"Testing File Suggestion Agent...\")\n",
    "    caller = await make_agent_caller(file_suggestion_agent, initial_state)\n",
    "    await caller.call(\"What files should we use?\")\n",
    "    await caller.call(\"Yes, approve those files\")\n",
    "    session = await caller.get_session()\n",
    "    return session.state\n",
    "\n",
    "# Uncomment to test individual components:\n",
    "# state1 = await test_user_intent_agent()\n",
    "# state2 = await test_file_suggestion_agent(state1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Customization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration options for the demo\n",
    "\n",
    "class DemoConfig:\n",
    "    \"\"\"Configuration class for the demo\"\"\"\n",
    "    \n",
    "    # LLM Configuration\n",
    "    MODEL = \"openai/gpt-4o\"\n",
    "    API_KEY = \"your-api-key-here\"  # Replace with your API key\n",
    "    \n",
    "    # Demo data\n",
    "    SAMPLE_FILES = [\n",
    "        'products.csv', \n",
    "        'suppliers.csv', \n",
    "        'parts.csv', \n",
    "        'part_supplier_mapping.csv', \n",
    "        'assemblies.csv',\n",
    "        'product_reviews/gothenburg_table_reviews.md',\n",
    "        'product_reviews/stockholm_chair_reviews.md'\n",
    "    ]\n",
    "    \n",
    "    # Use cases for suggestions\n",
    "    SAMPLE_USE_CASES = [\n",
    "        \"supply chain analysis\",\n",
    "        \"social network\",\n",
    "        \"recommendation system\",\n",
    "        \"fraud detection\",\n",
    "        \"logistics network\"\n",
    "    ]\n",
    "    \n",
    "    # Neo4j settings (for real deployment)\n",
    "    NEO4J_URI = \"bolt://localhost:7687\"\n",
    "    NEO4J_USERNAME = \"neo4j\"\n",
    "    NEO4J_PASSWORD = \"password\"\n",
    "    NEO4J_DATABASE = \"neo4j\"\n",
    "\n",
    "# You can modify the configuration above to customize the demo\n",
    "print(\"Demo configuration loaded. Modify DemoConfig class to customize the demo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates a complete end-to-end multi-agent workflow for knowledge graph construction. The system:\n",
    "\n",
    "1. **Captures user intent** through conversational AI\n",
    "2. **Suggests relevant files** based on the user's goals\n",
    "3. **Proposes schemas** for both structured and unstructured data\n",
    "4. **Constructs the knowledge graph** using the approved plans\n",
    "\n",
    "### Key Features:\n",
    "- **Multi-agent collaboration**: Each agent specializes in a specific aspect of the workflow\n",
    "- **State management**: Information flows seamlessly between agents\n",
    "- **User interaction**: Users can approve/reject suggestions at each step\n",
    "- **Flexible architecture**: Easy to extend with new agents or modify existing ones\n",
    "\n",
    "### To use this notebook:\n",
    "1. Update the `API_KEY` variable with your OpenAI API key\n",
    "2. For production use, replace the mock Neo4j implementation with real Neo4j connection\n",
    "3. Add your actual data files to the `data/` directory\n",
    "4. Run the complete workflow or test individual components\n",
    "\n",
    "This architecture provides a solid foundation for building sophisticated knowledge graph construction systems with AI agents!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}